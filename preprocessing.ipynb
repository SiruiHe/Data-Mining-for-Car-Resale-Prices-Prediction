{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "   listing_id                                              title  \\\n",
      "0     1292132  Land Rover Range Rover Velar 3.0A Si6 R-Dynami...   \n",
      "1     1294696   Mercedes-Benz C-Class C200 Sport Premium Sunroof   \n",
      "2     1311717              Honda Odyssey 2.4A (COE till 09/2027)   \n",
      "3     1310068       Toyota Corolla Altis 1.6A (COE till 12/2028)   \n",
      "4     1325280                     Lexus GS300 (COE till 06/2026)   \n",
      "\n",
      "            make    model                                        description  \\\n",
      "0     land rover    range  1 owner, no repairs needed! it looks great, in...   \n",
      "1  mercedes-benz     c200  rare beautiful white c200 sport premium sunroo...   \n",
      "2          honda  odyssey            comes with warranty. full service done.   \n",
      "3         toyota    altis                                                  0   \n",
      "4          lexus       gs  wear and tear done up. well maintained and reg...   \n",
      "\n",
      "   manufactured original_reg_date     reg_date  type_of_vehicle  \\\n",
      "0        2018.0               NaN  08-mar-2018              suv   \n",
      "1        2017.0               NaN  28-dec-2017     luxury sedan   \n",
      "2        2007.0               NaN  19-sep-2007              mpv   \n",
      "3        2008.0               NaN  15-dec-2008  mid-sized sedan   \n",
      "4        2006.0               NaN  22-dec-2006     luxury sedan   \n",
      "\n",
      "                                   category  ...   mileage      omv       arf  \\\n",
      "0                                  parf car  ...   96000.0  88906.0  132031.0   \n",
      "1                  parf car, premium ad car  ...   85680.0  40678.0   43950.0   \n",
      "2  coe car, premium ad car, low mileage car  ...  138000.0  27994.0   30794.0   \n",
      "3                   coe car, premium ad car  ...  160000.0  16084.0   16084.0   \n",
      "4                   coe car, premium ad car  ...  183000.0  50414.0   55456.0   \n",
      "\n",
      "  opc_scheme  lifespan   eco_category  \\\n",
      "0        NaN       NaN  uncategorized   \n",
      "1        NaN       NaN  uncategorized   \n",
      "2        NaN       NaN  uncategorized   \n",
      "3        NaN       NaN  uncategorized   \n",
      "4        NaN       NaN  uncategorized   \n",
      "\n",
      "                                            features  \\\n",
      "0  3l supercharged v6 p380 engine at 375bhp/450nm...   \n",
      "1  2.0l 4 cylinders inline turbocharged engine, p...   \n",
      "2  2.4l k24a 4 cylinders inline dohc i-vtec, 5 sp...   \n",
      "3  super fuel efficient 1.6l 16 valves dohc vvt-i...   \n",
      "4  powerful 3.0l v6 engine, 227bhp, 6 speed , key...   \n",
      "\n",
      "                                         accessories  indicative_price  \\\n",
      "0  2 x massage/memory/cooling & warmer seat, rear...               NaN   \n",
      "1  multi function steering, electric tailgate, re...               NaN   \n",
      "2  cruise control, touchscreen audio, reverse cam...               NaN   \n",
      "3  leather seats, pioneer dvd audio system with r...               NaN   \n",
      "4  premium upholstery electric seats. memory seat...               NaN   \n",
      "\n",
      "      price  \n",
      "0  193788.0  \n",
      "1   96800.0  \n",
      "2   39800.0  \n",
      "3   44800.0  \n",
      "4   25800.0  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Column names in dataset: Index(['listing_id', 'title', 'make', 'model', 'description', 'manufactured',\n",
      "       'original_reg_date', 'reg_date', 'type_of_vehicle', 'category',\n",
      "       'transmission', 'curb_weight', 'power', 'fuel_type', 'engine_cap',\n",
      "       'no_of_owners', 'depreciation', 'coe', 'road_tax', 'dereg_value',\n",
      "       'mileage', 'omv', 'arf', 'opc_scheme', 'lifespan', 'eco_category',\n",
      "       'features', 'accessories', 'indicative_price', 'price'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import MEstimateEncoder, TargetEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "train_file_path = \"./data/train.csv\"\n",
    "test_file_path = \"./data/test.csv\"\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "test = pd.read_csv(test_file_path)\n",
    "\n",
    "train, valid = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Print column names to confirm dataset structure\n",
    "print(\"Column names in dataset:\", train_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode_make(df, column, target):\n",
    "    \"\"\"Target encode the make column using MEstimateEncoder\"\"\"\n",
    "    encoder = MEstimateEncoder(\n",
    "        cols=[column],\n",
    "        m=5.0, \n",
    "    )\n",
    "    encoder.fit(df[[column]], df[target])\n",
    "    default_mean = df[target].mean()\n",
    "    return encoder, default_mean\n",
    "\n",
    "def apply_target_encoding(df, column, encoder, default_mean):\n",
    "    \"\"\"Apply encoder to the dataset\"\"\"\n",
    "    df_temp = df[[column]].copy()\n",
    "    encoded_values = encoder.transform(df_temp)\n",
    "    df[f\"{column}_target_encoded\"] = encoded_values[column]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categories_train(df, column_name):\n",
    "    \"\"\"Initialize and apply MultiLabelBinarizer on training data, return encoder for future use\"\"\"\n",
    "    df[f\"{column_name}_list\"] = df[column_name].apply(lambda x: x.split(', '))\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit_transform(df[f\"{column_name}_list\"])\n",
    "    return mlb\n",
    "\n",
    "def apply_categories_encoding(df, column_name, mlb):\n",
    "    \"\"\"Apply saved MultiLabelBinarizer to new DataFrame\"\"\"\n",
    "    df[f\"{column_name}_list\"] = df[column_name].apply(lambda x: x.split(', '))\n",
    "    df_encoded = mlb.transform(df[f\"{column_name}_list\"])\n",
    "    df_encoded = pd.DataFrame(df_encoded, columns=mlb.classes_, index=df.index)\n",
    "    df = pd.concat([df, df_encoded], axis=1)\n",
    "    df = df.drop(columns=[column_name, f\"{column_name}_list\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode_columns_train(df, columns):\n",
    "    \"\"\"Initialize and apply OneHotEncoder on training data, return encoders for future use\"\"\"\n",
    "    encoders = {}\n",
    "    for column in columns:\n",
    "        onehot_encoder = OneHotEncoder()\n",
    "        df_encoded = onehot_encoder.fit_transform(df[[column]])\n",
    "        encoders[column] = onehot_encoder\n",
    "    return encoders\n",
    "\n",
    "def apply_onehot_encoding(df, columns, encoders):\n",
    "    \"\"\"Apply saved OneHotEncoder to new DataFrame\"\"\"\n",
    "    for column in columns:\n",
    "        df_encoded = encoders[column].transform(df[[column]]).toarray()\n",
    "        df_encoded = pd.DataFrame(df_encoded, columns=encoders[column].get_feature_names_out([column]), index=df.index)\n",
    "        df = pd.concat([df, df_encoded], axis=1)\n",
    "        df = df.drop(columns=column)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Proecessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "del_cols = ['listing_id', 'original_reg_date','opc_scheme', 'lifespan','eco_category', 'indicative_price']\n",
    "text_cols = ['title', 'description', 'features', 'accessories']\n",
    "date_cols = ['reg_date']\n",
    "numeric_cols = ['manufactured', 'curb_weight', 'power', 'engine_cap', 'depreciation', 'coe', 'road_tax', \n",
    "            'dereg_value', 'mileage', 'omv', 'arf', 'year', 'month',\n",
    "            'text_brand_popularity_score', 'text_model_value_score', 'text_condition_score',\n",
    "            'text_feature_rarity_score', 'text_performance_score', 'text_sentiment_score']\n",
    "log_cols = ['manufactured', 'curb_weight', 'power_log', 'engine_cap_log', 'depreciation_log', 'coe', 'road_tax_log', 'dereg_value_log', 'mileage_log', 'omv_log', 'arf_log', 'year', 'month']\n",
    "root_cols = ['manufactured', 'curb_weight', 'power_root', 'engine_cap_root', 'depreciation_root', 'coe', 'road_tax_root', 'dereg_value_root', 'mileage_root', 'omv_root', 'arf_root', 'year', 'month']\n",
    "categorical_cols = ['make', 'model', 'type_of_vehicle', 'category', 'transmission', 'fuel_type', 'no_of_owners']\n",
    "\n",
    "def get_maxmin_dict(data, numeric_cols):\n",
    "    max_dict = dict()\n",
    "    min_dict = dict()\n",
    "    for feature in numeric_cols:\n",
    "        max_dict[feature] = data[feature].max()\n",
    "        min_dict[feature] = data[feature].min()\n",
    "    return max_dict, min_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_cat(data, del_cols, text_cols, target_encoder, default_mean, mlb_encoder, onehot_encoders, data_type='train'):\n",
    "    \"\"\"Process categorical features and merge GPT features\"\"\"\n",
    "    # Select GPT features file based on data type\n",
    "    if data_type == 'train':\n",
    "        gpt_features_file = 'data/with_text_features_train.csv'\n",
    "    else:\n",
    "        gpt_features_file = 'data/with_text_features_test.csv'\n",
    "        \n",
    "    if os.path.exists(gpt_features_file):\n",
    "        print(f\"Found GPT features file: {gpt_features_file}\")\n",
    "        gpt_features = pd.read_csv(gpt_features_file)\n",
    "        gpt_cols = ['listing_id', 'text_brand_popularity_score', 'text_model_value_score', \n",
    "                   'text_condition_score', 'text_feature_rarity_score', \n",
    "                   'text_performance_score', 'text_sentiment_score']\n",
    "        gpt_features = gpt_features[gpt_cols]\n",
    "        data = data.merge(gpt_features, on='listing_id', how='left')\n",
    "        \n",
    "        gpt_feature_cols = [col for col in gpt_cols if col != 'listing_id']\n",
    "        for col in gpt_feature_cols:\n",
    "            data[col] = data[col].fillna(0.5)\n",
    "    else:\n",
    "        print(f\"GPT features file not found: {gpt_features_file}, skipping GPT features merge\")\n",
    "    \n",
    "    data = data.drop(columns=del_cols)\n",
    "    data = data.drop(columns=text_cols)\n",
    "    \n",
    "    data = apply_target_encoding(data, 'make', target_encoder, default_mean)\n",
    "    data = apply_categories_encoding(data, 'category', mlb_encoder)\n",
    "    data = apply_onehot_encoding(data, ['type_of_vehicle', 'fuel_type', 'transmission'], onehot_encoders)\n",
    "\n",
    "    data['reg_date'] = pd.to_datetime(data['reg_date'], format='%d-%b-%Y')  \n",
    "    data['year'] = data['reg_date'].dt.year\n",
    "    data['month'] = data['reg_date'].dt.month\n",
    "    data = data.drop(columns='reg_date')\n",
    "    data['no_of_owners'] = data['no_of_owners'].fillna(2)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_data_num(data, max_dict, min_dict, \n",
    "                       remove_outliers=False,\n",
    "                       do_normalize=False, \n",
    "                       normalize_method='standard'):\n",
    "    \"\"\"Process numeric features\"\"\"\n",
    "    for feature in numeric_cols:\n",
    "        data[feature] = data[feature].fillna(data[feature].median())\n",
    "    \n",
    "    mask = ~((data[numeric_cols] - data[numeric_cols].mean()).abs() > 3 * data[numeric_cols].std()).any(axis=1)\n",
    "    \n",
    "    if remove_outliers:\n",
    "        data = data[mask]\n",
    "    \n",
    "    long_tail_features = ['omv', 'arf', 'depreciation', 'dereg_value', 'power', 'engine_cap', 'road_tax', 'mileage']\n",
    "    for feature in long_tail_features:\n",
    "        data[f'{feature}_log'] = np.log1p(data[feature])\n",
    "        data[f'{feature}_root'] = np.sqrt(data[feature])\n",
    "\n",
    "    if do_normalize:\n",
    "        for feature in numeric_cols:\n",
    "            if normalize_method == 'standard':\n",
    "                max_dict[f\"{feature}_mean\"] = data[feature].mean()\n",
    "                min_dict[f\"{feature}_std\"] = data[feature].std()\n",
    "                data[feature] = (data[feature] - max_dict[f\"{feature}_mean\"]) / min_dict[f\"{feature}_std\"]\n",
    "            else:\n",
    "                max_dict[feature] = data[feature].max()\n",
    "                min_dict[feature] = data[feature].min()\n",
    "                data[feature] = (data[feature] - min_dict[feature]) / (max_dict[feature] - min_dict[feature])\n",
    "        \n",
    "        for feature in long_tail_features:\n",
    "            log_name = f'{feature}_log'\n",
    "            root_name = f'{feature}_root'\n",
    "            \n",
    "            if normalize_method == 'standard':\n",
    "                max_dict[f\"{log_name}_mean\"] = data[log_name].mean()\n",
    "                min_dict[f\"{log_name}_std\"] = data[log_name].std()\n",
    "                max_dict[f\"{root_name}_mean\"] = data[root_name].mean()\n",
    "                min_dict[f\"{root_name}_std\"] = data[root_name].std()\n",
    "                \n",
    "                data[log_name] = (data[log_name] - max_dict[f\"{log_name}_mean\"]) / min_dict[f\"{log_name}_std\"]\n",
    "                data[root_name] = (data[root_name] - max_dict[f\"{root_name}_mean\"]) / min_dict[f\"{root_name}_std\"]\n",
    "            else:\n",
    "                max_dict[log_name] = data[log_name].max()\n",
    "                min_dict[log_name] = data[log_name].min()\n",
    "                max_dict[root_name] = data[root_name].max()\n",
    "                min_dict[root_name] = data[root_name].min()\n",
    "                \n",
    "                data[log_name] = (data[log_name] - min_dict[log_name]) / (max_dict[log_name] - min_dict[log_name])\n",
    "                data[root_name] = (data[root_name] - min_dict[root_name]) / (max_dict[root_name] - min_dict[root_name])\n",
    "    \n",
    "    return data, mask if remove_outliers else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1: Processing training data...\n",
      "Found GPT features file: data/with_text_features_train.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPT features file: data/with_text_features_train.csv\n",
      "Training data saved to data/processed/ directory\n",
      "Part 2: Processing full dataset...\n",
      "Found GPT features file: data/with_text_features_train.csv\n",
      "Found GPT features file: data/with_text_features_test.csv\n",
      "Full training data and test data saved to data/processed/ directory\n"
     ]
    }
   ],
   "source": [
    "do_normalize = False\n",
    "normalize_method='minmax'\n",
    "\n",
    "# Part 1: Process training data\n",
    "print(\"Part 1: Processing training data...\")\n",
    "X_train, y_train = train.drop(columns=['price']), train['price']\n",
    "X_valid, y_valid = valid.drop(columns=['price']), valid['price']\n",
    "\n",
    "# Generate encoders using training data\n",
    "target_encoder_train, default_mean_train = target_encode_make(train, 'make', 'price')\n",
    "mlb_encoder_train = encode_categories_train(train, 'category')\n",
    "onehot_encoders_train = onehot_encode_columns_train(train, ['type_of_vehicle', 'fuel_type', 'transmission'])\n",
    "\n",
    "# Process categorical features\n",
    "X_train = preprocess_data_cat(X_train, del_cols, text_cols, target_encoder_train, default_mean_train, mlb_encoder_train, onehot_encoders_train)\n",
    "X_valid = preprocess_data_cat(X_valid, del_cols, text_cols, target_encoder_train, default_mean_train, mlb_encoder_train, onehot_encoders_train)\n",
    "\n",
    "# Process numerical features\n",
    "max_dict_train, min_dict_train = get_maxmin_dict(X_train, numeric_cols)\n",
    "X_train, mask = preprocess_data_num(X_train, max_dict_train, min_dict_train, do_normalize=do_normalize, normalize_method=normalize_method, remove_outliers=False)\n",
    "if mask is not None:\n",
    "    y_train = y_train[mask]\n",
    "X_valid, _ = preprocess_data_num(X_valid, max_dict_train, min_dict_train, do_normalize=do_normalize, normalize_method=normalize_method, remove_outliers=False)\n",
    "\n",
    "# Save training data\n",
    "import os\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "X_train.to_csv('data/processed/X_train.csv', index=False)\n",
    "y_train.to_csv('data/processed/y_train.csv', index=False)\n",
    "X_valid.to_csv('data/processed/X_valid.csv', index=False)\n",
    "y_valid.to_csv('data/processed/y_valid.csv', index=False)\n",
    "print('Training data saved to data/processed/ directory')\n",
    "\n",
    "# Part 2: Process full dataset\n",
    "print(\"Part 2: Processing full dataset...\")\n",
    "X_test = test\n",
    "\n",
    "# 1. Process full training data first\n",
    "X_train_full = train_df.drop(columns=['price'])\n",
    "y_train_full = train_df['price']\n",
    "\n",
    "# 2. Generate encoders using full training data\n",
    "target_encoder_full, default_mean_full = target_encode_make(train_df, 'make', 'price')\n",
    "mlb_encoder_full = encode_categories_train(train_df, 'category')\n",
    "onehot_encoders_full = onehot_encode_columns_train(train_df, ['type_of_vehicle', 'fuel_type', 'transmission'])\n",
    "\n",
    "# 3. Process categorical features\n",
    "X_train_full = preprocess_data_cat(X_train_full, del_cols, text_cols, target_encoder_full, default_mean_full, mlb_encoder_full, onehot_encoders_full)\n",
    "X_test = preprocess_data_cat(X_test, del_cols, text_cols, target_encoder_full, default_mean_full, mlb_encoder_full, onehot_encoders_full, data_type='test')\n",
    "\n",
    "# 4. Get max/min dictionary using processed full training data\n",
    "max_dict_full, min_dict_full = get_maxmin_dict(X_train_full, numeric_cols)\n",
    "\n",
    "# 5. Process numerical features\n",
    "X_train_full, _ = preprocess_data_num(X_train_full, max_dict_full, min_dict_full, do_normalize=do_normalize, normalize_method=normalize_method, remove_outliers=False)\n",
    "X_test, _ = preprocess_data_num(X_test, max_dict_full, min_dict_full, do_normalize=do_normalize, normalize_method=normalize_method, remove_outliers=False)\n",
    "\n",
    "# Save processed full training data and test data\n",
    "X_train_full.to_csv('data/processed/X_train_full.csv', index=False)\n",
    "y_train_full.to_csv('data/processed/y_train_full.csv', index=False)\n",
    "X_test.to_csv('data/processed/X_test.csv', index=False)\n",
    "print('Full training data and test data saved to data/processed/ directory')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
