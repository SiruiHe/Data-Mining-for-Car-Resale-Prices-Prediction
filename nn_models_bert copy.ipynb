{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集读取完成\n",
      "训练集大小: (20000, 74)\n",
      "验证集大小: (5000, 74)\n",
      "全量训练集大小: (25000, 74)\n",
      "测试集大小: (10000, 74)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import json\n",
    "import math\n",
    "\n",
    "# 读取处理后的数据集\n",
    "X_train = pd.read_csv('data/processed/X_train.csv')\n",
    "y_train = pd.read_csv('data/processed/y_train.csv')\n",
    "X_valid = pd.read_csv('data/processed/X_valid.csv')\n",
    "y_valid = pd.read_csv('data/processed/y_valid.csv') \n",
    "X_train_full = pd.read_csv('data/processed/X_train_full.csv') \n",
    "y_train_full = pd.read_csv('data/processed/y_train_full.csv') \n",
    "X_test = pd.read_csv('data/processed/X_test.csv')\n",
    "\n",
    "print('数据集读取完成')\n",
    "print(f'训练集大小: {X_train.shape}')\n",
    "print(f'验证集大小: {X_valid.shape}')\n",
    "print(f'全量训练集大小: {X_train_full.shape}')\n",
    "print(f'测试集大小: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: (20000, 13)\n",
      "全量集: (25000, 13)\n",
      "测试集: (10000, 13)\n"
     ]
    }
   ],
   "source": [
    "# 直接定义列名配置\n",
    "del_cols = ['listing_id', 'original_reg_date', 'opc_scheme', 'lifespan', 'eco_category', 'indicative_price']\n",
    "text_cols = ['title', 'description', 'features', 'accessories']\n",
    "date_cols = ['reg_date']\n",
    "numeric_cols = ['manufactured', 'curb_weight', 'power', 'engine_cap', 'depreciation', 'coe', 'road_tax', \n",
    "                'dereg_value', 'mileage', 'omv', 'arf', 'year', 'month',\n",
    "                'text_brand_popularity_score', 'text_model_value_score', 'text_condition_score',\n",
    "                'text_feature_rarity_score', 'text_performance_score', 'text_sentiment_score']\n",
    "log_cols = ['manufactured', 'curb_weight', 'power_log', 'engine_cap_log', 'depreciation_log', 'coe', \n",
    "            'road_tax_log', 'dereg_value_log', 'mileage_log', 'omv_log', 'arf_log', 'year', 'month']\n",
    "root_cols = ['manufactured', 'curb_weight', 'power_root', 'engine_cap_root', 'depreciation_root', 'coe', \n",
    "             'road_tax_root', 'dereg_value_root', 'mileage_root', 'omv_root', 'arf_root', 'year', 'month']\n",
    "categorical_cols = ['make', 'model', 'type_of_vehicle', 'category', 'transmission', 'fuel_type', 'no_of_owners']\n",
    "\n",
    "# 更新变换列，添加GPT特征\n",
    "cat_nu_cols = [\n",
    "    \"manufactured\", \"curb_weight\", \"power\", \"engine_cap\", \"no_of_owners\", \"depreciation\", \n",
    "    \"coe\", \"road_tax\", \"dereg_value\", \"mileage\", \"omv\", \"arf\", \"make_target_encoded\",\n",
    "]\n",
    "\n",
    "cat_log_cols = [\n",
    "    \"manufactured\", \"curb_weight\", \"power_log\", \"engine_cap_log\", \"depreciation_log\", \n",
    "    \"coe\", \"road_tax_log\", \"dereg_value_log\", \"mileage_log\", \"omv_log\", \"arf_log\", \n",
    "    \"make_target_encoded\", \"text_brand_popularity_score\", \"text_model_value_score\", \n",
    "    \"text_condition_score\", \"text_feature_rarity_score\", \"text_performance_score\", \n",
    "    \"text_sentiment_score\", \"-\", \"almost new car\", \"coe car\", \"consignment car\", \n",
    "    \"direct owner sale\", \"electric cars\", \"hybrid cars\", \"imported used vehicle\", \n",
    "    \"low mileage car\", \"opc car\", \"parf car\", \"premium ad car\", \"rare & exotic\", \n",
    "    \"sgcarmart warranty cars\", \"sta evaluated car\", \"vintage cars\", \n",
    "    \"type_of_vehicle_bus/mini bus\", \"type_of_vehicle_hatchback\", \n",
    "    \"type_of_vehicle_luxury sedan\", \"type_of_vehicle_mid-sized sedan\", \n",
    "    \"type_of_vehicle_mpv\", \"type_of_vehicle_others\", \"type_of_vehicle_sports car\", \n",
    "    \"type_of_vehicle_stationwagon\", \"type_of_vehicle_suv\", \"type_of_vehicle_truck\", \n",
    "    \"type_of_vehicle_van\", \"fuel_type_diesel\", \"fuel_type_diesel-electric\", \n",
    "    \"fuel_type_electric\", \"fuel_type_petrol\", \"fuel_type_petrol-electric\", \n",
    "    \"fuel_type_nan\", \"transmission_manual\", \"year\", \"month\"\n",
    "]\n",
    "\n",
    "cat_root_cols = [\n",
    "    \"manufactured\", \"curb_weight\", \"power_root\", \"engine_cap_root\", \"depreciation_root\", \n",
    "    \"coe\", \"road_tax_root\", \"dereg_value_root\", \"mileage_root\", \"omv_root\", \"arf_root\", \n",
    "    \"make_target_encoded\", \"text_brand_popularity_score\", \"text_model_value_score\", \n",
    "    \"text_condition_score\", \"text_feature_rarity_score\", \"text_performance_score\", \n",
    "    \"text_sentiment_score\", \"-\", \"almost new car\", \"coe car\", \"consignment car\", \n",
    "    \"direct owner sale\", \"electric cars\", \"hybrid cars\", \"imported used vehicle\", \n",
    "    \"low mileage car\", \"opc car\", \"parf car\", \"premium ad car\", \"rare & exotic\", \n",
    "    \"sgcarmart warranty cars\", \"sta evaluated car\", \"vintage cars\", \n",
    "    \"type_of_vehicle_bus/mini bus\", \"type_of_vehicle_hatchback\", \n",
    "    \"type_of_vehicle_luxury sedan\", \"type_of_vehicle_mid-sized sedan\", \n",
    "    \"type_of_vehicle_mpv\", \"type_of_vehicle_others\", \"type_of_vehicle_sports car\", \n",
    "    \"type_of_vehicle_stationwagon\", \"type_of_vehicle_suv\", \"type_of_vehicle_truck\", \n",
    "    \"type_of_vehicle_van\", \"fuel_type_diesel\", \"fuel_type_diesel-electric\", \n",
    "    \"fuel_type_electric\", \"fuel_type_petrol\", \"fuel_type_petrol-electric\", \n",
    "    \"fuel_type_nan\", \"transmission_manual\", \"year\", \"month\"\n",
    "]\n",
    "\n",
    "# 丢弃log和root变换的结果\n",
    "X_train = X_train[cat_nu_cols]\n",
    "X_valid = X_valid[cat_nu_cols]\n",
    "X_test = X_test[cat_nu_cols]\n",
    "X_train_full = X_train_full[cat_nu_cols]\n",
    "\n",
    "print(f'训练集: {X_train.shape}')\n",
    "print(f'全量集: {X_train_full.shape}')\n",
    "print(f'测试集: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "X_valid = X_valid.values\n",
    "X_train_full = X_train_full.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10021\\anaconda3\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度:\n",
      "原始特征: 8\n",
      "UMAP特征: 8\n",
      "组合特征: 16\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from umap.umap_ import UMAP  # 正确的导入方式\n",
    "\n",
    "# # 加载BERT向量\n",
    "# bert_train_vectors = np.load('data/processed/train_vectors.npy')\n",
    "# bert_valid_vectors = np.load('data/processed/valid_vectors.npy')\n",
    "# bert_train_full_vectors = np.load('data/processed/train_full_vectors.npy')\n",
    "# bert_test_vectors = np.load('data/processed/test_vectors.npy')\n",
    "\n",
    "# # BERT降维\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # UMAP参数设置\n",
    "# umap = UMAP(\n",
    "#     n_components=8,\n",
    "#     n_neighbors=20,\n",
    "#     min_dist=0.1,\n",
    "#     metric='cosine',\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # umap = UMAP(\n",
    "# #     n_components=16,\n",
    "# #     n_neighbors=30,\n",
    "# #     min_dist=0.3,\n",
    "# #     metric='cosine',\n",
    "# #     random_state=42\n",
    "# # )\n",
    "\n",
    "# # umap = UMAP(\n",
    "# #     n_components=24,\n",
    "# #     n_neighbors=50,\n",
    "# #     min_dist=0.5,\n",
    "# #     metric='cosine',\n",
    "# #     random_state=42\n",
    "# # )\n",
    "\n",
    "# # 对BERT向量进行UMAP降维\n",
    "# bert_train_scaled = scaler.fit_transform(bert_train_vectors)\n",
    "# bert_train_umap = umap.fit_transform(bert_train_scaled)\n",
    "\n",
    "# # 对验证集和测试集应用相同的转换\n",
    "# bert_valid_scaled = scaler.transform(bert_valid_vectors)\n",
    "# bert_valid_umap = umap.transform(bert_valid_scaled)\n",
    "\n",
    "# bert_test_scaled = scaler.transform(bert_test_vectors)\n",
    "# bert_test_umap = umap.transform(bert_test_scaled)\n",
    "\n",
    "# # 对完整训练集进行转换\n",
    "# bert_train_full_scaled = scaler.transform(bert_train_full_vectors)\n",
    "# bert_train_full_umap = umap.transform(bert_train_full_scaled)\n",
    "\n",
    "# # 拼接特征\n",
    "# X_train_combined = np.hstack((X_train[cat_nu_cols].values, bert_train_umap))\n",
    "# X_valid_combined = np.hstack((X_valid[cat_nu_cols].values, bert_valid_umap))\n",
    "# X_test_combined = np.hstack((X_test[cat_nu_cols].values, bert_test_umap))\n",
    "# X_train_full_combined = np.hstack((X_train_full[cat_nu_cols].values, bert_train_full_umap))\n",
    "\n",
    "\n",
    "# # 打印维度信息\n",
    "# print(\"特征维度:\")\n",
    "# print(f\"原始特征: {X_train[cat_nu_cols].shape[1]}\")\n",
    "# print(f\"UMAP特征: {bert_train_umap.shape[1]}\")\n",
    "# print(f\"组合特征: {X_train_combined.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 16)\n",
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "# print(X_train_full_combined.shape)\n",
    "# print(X_test_combined.shape)\n",
    "# X_train = X_train_combined\n",
    "# X_valid = X_valid_combined\n",
    "# X_test = X_test_combined\n",
    "# X_train_full = X_train_full_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, lr=0.001, wd=0.1, seed=42):\n",
    "    set_seed(seed)  # Set the random seed for reproducibility    \n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "    X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "    y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    epochs = 100\n",
    "    best_rmse = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test_pred_mlp = model(X_valid_tensor)\n",
    "            mse_test_mlp = mean_squared_error(y_valid_tensor.numpy(), y_test_pred_mlp.numpy())\n",
    "            rmse_test_mlp = np.sqrt(mse_test_mlp)\n",
    "\n",
    "        # 更新最佳RMSE\n",
    "        if rmse_test_mlp < best_rmse:\n",
    "            best_rmse = rmse_test_mlp\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # 训练流程结束后打印最佳RMSE\n",
    "    print(f'Best Valid RMSE: {best_rmse:.4f}')\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "print(input_dim)\n",
    "        \n",
    "# input_dim = len(cat_nu_cols)\n",
    "# print(input_dim)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 27504.1172\n"
     ]
    }
   ],
   "source": [
    "# model = MLPModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.01)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001)\n",
    "# model = MLPModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 26967.7324\n"
     ]
    }
   ],
   "source": [
    "model = DeeperMLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MLPModel(input_dim)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_nu_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.002\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m MLPModel(input_dim)\n\u001b[0;32m      4\u001b[0m train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,\u001b[38;5;241m0.0005\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 35\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, lr, wd, seed)\u001b[0m\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     34\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 35\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3365\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3363\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3365\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.002)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 25477.8066\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 24372.8418\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 24115.1914\n"
     ]
    }
   ],
   "source": [
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 1)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 1e-1)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Valid RMSE: 23797.7793\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 26231.5938\n"
     ]
    }
   ],
   "source": [
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.2)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MLPModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0)\n",
    "# model = MLPModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.0001)\n",
    "# model = MLPModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.001)\n",
    "# model = MLPModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.01)\n",
    "# model = MLPModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithBertModel(nn.Module):\n",
    "    def __init__(self, input_dim, bert_dim=8, reduced_dim=16):\n",
    "        super(MLPWithBertModel, self).__init__()\n",
    "        # Assuming the BERT output is at the last part of the input\n",
    "        self.bert_processor = nn.Sequential(\n",
    "            nn.Linear(bert_dim, reduced_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # New input dimension after concatenating reduced BERT output\n",
    "        new_input_dim = input_dim - bert_dim + reduced_dim\n",
    "        \n",
    "        self.layer1 = nn.Linear(new_input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Assuming x is the input where the last 8 elements are the BERT vector\n",
    "        bert_vector = x[:, -8:]  # Extract the last 8 dimensions\n",
    "        other_features = x[:, :-8]  # Extract all other features\n",
    "        \n",
    "        # Process the BERT vector\n",
    "        processed_bert = self.bert_processor(bert_vector)\n",
    "        \n",
    "        # Concatenate the processed BERT output with other features\n",
    "        x = torch.cat((other_features, processed_bert), dim=1)\n",
    "        \n",
    "        # Feed through the subsequent layers\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 27253.2637\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 28563.5703\n"
     ]
    }
   ],
   "source": [
    "# model = MLPWithBertModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.01)\n",
    "model = MLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001)\n",
    "model = MLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 26958.0684\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 27811.0312\n"
     ]
    }
   ],
   "source": [
    "model = MLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.002)\n",
    "model = MLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 25443.4902\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 24840.1016\n"
     ]
    }
   ],
   "source": [
    "model = MLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = MLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDropoutModel(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.5):\n",
    "        super(MLPDropoutModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Valid RMSE: 37516.0312\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 47186.6484\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 40783.5469\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 45948.7266\n"
     ]
    }
   ],
   "source": [
    "model = MLPDropoutModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = MLPDropoutModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = MLPDropoutModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = MLPDropoutModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 30619.6211\n",
      "Best Valid RMSE: 32595.4590\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 29705.5312\n",
      "Best Valid RMSE: 32272.6719\n"
     ]
    }
   ],
   "source": [
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 30197.6641\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 30638.0859\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 30579.2617\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 31158.3945\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 31164.0371\n"
     ]
    }
   ],
   "source": [
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.0001)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.001)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.01)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPBNModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPBNModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)  # Batch normalization layer for the first layer output\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)   # Batch normalization layer for the second layer output\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)   # Batch normalization layer for the third layer output\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.layer1(x)))  # Apply BatchNorm after linear transformation, before ReLU\n",
    "        x = self.relu(self.bn2(self.layer2(x)))\n",
    "        x = self.relu(self.bn3(self.layer3(x)))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.01)  # Initialize biases to a small constant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Valid RMSE: 142090.3438\n",
      "Best Valid RMSE: 182349.1562\n",
      "Best Valid RMSE: 142823.1719\n",
      "Best Valid RMSE: 182349.1562\n"
     ]
    }
   ],
   "source": [
    "model = MLPBNModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = MLPBNModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = MLPBNModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.1)\n",
    "model = MLPBNModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperMLPModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeeperMLPModel, self).__init__()\n",
    "        # Increasing the depth with more layers\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.layer3 = nn.Linear(128, 128)\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.layer5 = nn.Linear(64, 64)\n",
    "        self.layer6 = nn.Linear(64, 32)\n",
    "        self.layer7 = nn.Linear(32, 16)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights with Kaiming initialization suitable for ReLU\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.relu(self.layer5(x))\n",
    "        x = self.relu(self.layer6(x))\n",
    "        x = self.relu(self.layer7(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        # Apply Kaiming He initialization to all linear layers in the model\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 27650.2754\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, \u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m DeeperMLPModel(input_dim)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_nu_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m DeeperMLPModel(input_dim)\n\u001b[0;32m      6\u001b[0m train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, lr, wd, seed)\u001b[0m\n\u001b[0;32m     34\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     35\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = DeeperMLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = DeeperMLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001)\n",
    "model = DeeperMLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = DeeperMLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperMLPWithBertModel(nn.Module):\n",
    "    def __init__(self, input_dim, bert_dim=8, reduced_bert_dim=128):\n",
    "        super(DeeperMLPWithBertModel, self).__init__()\n",
    "        \n",
    "        # Processing the BERT vector\n",
    "        self.bert_processor = nn.Sequential(\n",
    "            nn.Linear(bert_dim, reduced_bert_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # New input dimension after reducing the BERT vector and concatenating it back\n",
    "        new_input_dim = input_dim - bert_dim + reduced_bert_dim\n",
    "\n",
    "        # Increasing the depth with more layers\n",
    "        self.layer1 = nn.Linear(new_input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.layer3 = nn.Linear(128, 128)\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.layer5 = nn.Linear(64, 64)\n",
    "        self.layer6 = nn.Linear(64, 32)\n",
    "        self.layer7 = nn.Linear(32, 16)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights with Kaiming initialization suitable for ReLU\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split the input into BERT vector and other features\n",
    "        bert_vector = x[:, -8:]  # Assuming BERT vector is the last 8 elements\n",
    "        other_features = x[:, :-8]  # The rest of the features\n",
    "\n",
    "        # Process the BERT vector\n",
    "        processed_bert = self.bert_processor(bert_vector)\n",
    "\n",
    "        # Concatenate the processed BERT vector with other features\n",
    "        x = torch.cat((other_features, processed_bert), dim=1)\n",
    "\n",
    "        # Sequentially process through all layers\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.relu(self.layer5(x))\n",
    "        x = self.relu(self.layer6(x))\n",
    "        x = self.relu(self.layer7(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 26706.6309\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 23794.9082\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 24500.0039\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 26792.1055\n"
     ]
    }
   ],
   "source": [
    "model = DeeperMLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = DeeperMLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001)\n",
    "model = DeeperMLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = DeeperMLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepResidualMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepResidualMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, 256)\n",
    "        self.layer4 = nn.Linear(256, 256)\n",
    "        self.layer5 = nn.Linear(256, 256)\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights with He initialization suitable for ReLU\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.relu(self.layer1(x))\n",
    "        \n",
    "        # Residual block 1\n",
    "        out = self.relu(self.layer2(x))\n",
    "        x = out + x  # Changed from 'out += x' to 'x = out + x'\n",
    "    \n",
    "        # Residual block 2\n",
    "        out = self.relu(self.layer3(x))\n",
    "        x = out + x  # Changed from 'x += out' to 'x = out + x'\n",
    "    \n",
    "        # Residual block 3\n",
    "        out = self.relu(self.layer4(x))\n",
    "        x = out + x  # Changed from 'out += x' to 'x = out + x'\n",
    "    \n",
    "        # Residual block 4\n",
    "        out = self.relu(self.layer5(x))\n",
    "        x = out + x  # Changed from 'x += out' to 'x = out + x'\n",
    "    \n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 23950.7910\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 28136.4883\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 26597.7148\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 28391.6035\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.1)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 256]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m DeepResidualMLP(input_dim)\n\u001b[1;32m---> 45\u001b[0m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_nu_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[118], line 36\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, lr, wd, seed)\u001b[0m\n\u001b[0;32m     34\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     35\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     39\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 256]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "class DeepResidualMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepResidualMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, 256)\n",
    "        self.layer4 = nn.Linear(256, 256)\n",
    "        self.layer5 = nn.Linear(256, 256)\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights with He initialization suitable for ReLU\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input pass\n",
    "        identity = x\n",
    "        x = self.relu(self.layer1(x))\n",
    "        \n",
    "        # Residual block 1\n",
    "        out = self.relu(self.layer2(x))\n",
    "        out += x  # Adding input after the block\n",
    "        \n",
    "        # Residual block 2\n",
    "        x = self.relu(self.layer3(out))\n",
    "        x += out  # Adding input from the previous block\n",
    "        \n",
    "        # Residual block 3\n",
    "        out = self.relu(self.layer4(x))\n",
    "        out += x  # Adding input from the previous block\n",
    "        \n",
    "        # Residual block 4\n",
    "        x = self.relu(self.layer5(out))\n",
    "        x += out  # Adding input from the previous block\n",
    "\n",
    "        # Output pass\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# model = DeepResidualMLP(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        self.key_layer = nn.Linear(feature_dim, feature_dim, bias=False)\n",
    "        self.query_layer = nn.Linear(feature_dim, feature_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query_layer(x).unsqueeze(1)  # Adding batch dimension\n",
    "        key = self.key_layer(x).unsqueeze(-1)  # Adding an extra dimension for bmm\n",
    "        \n",
    "        # Compute attention scores and apply softmax\n",
    "        scores = torch.bmm(query, key)  # Should work as both are 3D now\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply weights to the original input features, using batch matrix multiplication\n",
    "        attended = torch.bmm(weights, x.unsqueeze(1))  # x also needs to be 3D\n",
    "        return attended.squeeze(1)  # Remove the extra dimension to match expected output shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeepResidualMLPWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepResidualMLPWithAttention, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, 256)\n",
    "        self.layer4 = nn.Linear(256, 256)\n",
    "        self.layer5 = nn.Linear(256, 256)\n",
    "        self.attention = Attention(256)  # Attention layer after layer5\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.relu(self.layer1(x))\n",
    "        \n",
    "        out = self.relu(self.layer2(x))\n",
    "        out = out + x  # Use out-of-place operation\n",
    "\n",
    "        x = self.relu(self.layer3(out))\n",
    "        x = x + out  # Use out-of-place operation\n",
    "\n",
    "        out = self.relu(self.layer4(x))\n",
    "        out = out + x  # Use out-of-place operation\n",
    "\n",
    "        x = self.relu(self.layer5(out))\n",
    "        x = x + out  # Use out-of-place operation\n",
    "\n",
    "        # Apply attention\n",
    "        x = self.attention(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepResidualMLPWithAttention(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = DeepResidualMLPWithAttention(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = DeepResidualMLPWithAttention(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = DeepResidualMLPWithAttention(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 26804.3965\n",
      "Validation results saved to data/nn_valid.csv\n",
      "Test results saved to data/nn_test.csv\n"
     ]
    }
   ],
   "source": [
    "def train_network_and_get_result(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, X_test, lr=0.001, wd=0, seed=42):\n",
    "    set_seed(seed)  # Set the random seed for reproducibility    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "    X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "    y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    epochs = 100\n",
    "    best_rmse = float('inf')\n",
    "    best_model = None\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_valid_pred = model(X_valid_tensor).numpy().flatten()\n",
    "            mse_valid = mean_squared_error(y_valid_tensor.numpy(), y_valid_pred)\n",
    "            rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "        if rmse_valid < best_rmse:\n",
    "            best_rmse = rmse_valid\n",
    "            best_model = model\n",
    "            torch.save(best_model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(f'Best Valid RMSE: {best_rmse:.4f}')\n",
    "\n",
    "    # Saving validation predictions to a CSV file\n",
    "    valid_predictions_df = pd.DataFrame({\n",
    "        'Id': range(len(y_valid_pred)),\n",
    "        'Predicted': y_valid_pred\n",
    "    })\n",
    "    valid_predictions_df.to_csv('data/nn_valid.csv', index=False)\n",
    "    print(\"Validation results saved to data/nn_valid.csv\")\n",
    "\n",
    "    # Saving test predictions to a CSV file if the best model was found\n",
    "    if best_model:\n",
    "        best_model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = best_model(X_test_tensor).numpy().flatten()\n",
    "\n",
    "        test_predictions_df = pd.DataFrame({\n",
    "            'Id': range(len(y_test_pred)),\n",
    "            'Predicted': y_test_pred\n",
    "        })\n",
    "        test_predictions_df.to_csv('data/nn_test.csv', index=False)\n",
    "        print(\"Test results saved to data/nn_test.csv\")\n",
    "\n",
    "model = DeeperMLPModel(input_dim)\n",
    "train_network_and_get_result(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, X_test,0.001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 26591.4785\n",
      "Validation results saved to data/full_nn_valid.csv\n",
      "Test results saved to data/full_nn_test.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def full_train_network_and_get_result(model, cat_nu_cols, X_train_full, y_train_full, X_test, lr=0.001, wd=0, seed=42):\n",
    "    set_seed(seed)  # Set the random seed for reproducibility    \n",
    "    train_indices, valid_indices = train_test_split(np.arange(len(X_train_full)), test_size=0.2, random_state=42)\n",
    "    X_train = X_train_full[train_indices]\n",
    "    y_train = y_train_full.iloc[train_indices]\n",
    "    X_valid = X_train_full[valid_indices]\n",
    "    y_valid = y_train_full.iloc[valid_indices]\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "    X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "    y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    epochs = 100\n",
    "    best_rmse = float('inf')\n",
    "    best_model = None\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_valid_pred = model(X_valid_tensor).numpy().flatten()\n",
    "            mse_valid = mean_squared_error(y_valid_tensor.numpy(), y_valid_pred)\n",
    "            rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "        if rmse_valid < best_rmse:\n",
    "            best_rmse = rmse_valid\n",
    "            best_model = model\n",
    "            torch.save(best_model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(f'Best Valid RMSE: {best_rmse:.4f}')\n",
    "\n",
    "    # Saving validation predictions to a CSV file\n",
    "    valid_predictions_df = pd.DataFrame({\n",
    "        'Id': range(len(y_valid_pred)),\n",
    "        'Predicted': y_valid_pred\n",
    "    })\n",
    "    valid_predictions_df.to_csv('data/full_nn_valid.csv', index=False)\n",
    "    print(\"Validation results saved to data/full_nn_valid.csv\")\n",
    "\n",
    "    # Saving test predictions to a CSV file if the best model was found\n",
    "    if best_model:\n",
    "        best_model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = best_model(X_test_tensor).numpy().flatten()\n",
    "\n",
    "        test_predictions_df = pd.DataFrame({\n",
    "            'Id': range(len(y_test_pred)),\n",
    "            'Predicted': y_test_pred\n",
    "        })\n",
    "        test_predictions_df.to_csv('data/full_nn_test.csv', index=False)\n",
    "        print(\"Test results saved to data/full_nn_test.csv\")\n",
    "        \n",
    "model = DeeperMLPModel(input_dim)\n",
    "full_train_network_and_get_result(model, cat_nu_cols, X_train_full, y_train_full , X_test,0.001, 0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 选择表现最好的模型进行最终训练和预测\n",
    "# best_model = DeeperMLPModel(input_dim)\n",
    "\n",
    "# print(\"使用全量数据训练最终模型...\")\n",
    "# # 转换数据为tensor\n",
    "# X_train_full_tensor = torch.tensor(X_train_full[cat_nu_cols].values, dtype=torch.float32)\n",
    "# y_train_full_tensor = torch.tensor(y_train_full.values, dtype=torch.float32).view(-1, 1)\n",
    "# train_dataset = TensorDataset(X_train_full_tensor, y_train_full_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# # 训练模型\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
    "\n",
    "# epochs = 50\n",
    "# for epoch in range(epochs):\n",
    "#     best_model.train()\n",
    "#     for inputs, targets in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = best_model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# print(\"生成测试集预测结果...\")\n",
    "# best_model.eval()\n",
    "# X_test_tensor = torch.tensor(X_test[cat_nu_cols].values, dtype=torch.float32)\n",
    "# with torch.no_grad():\n",
    "#     test_predictions = best_model(X_test_tensor).numpy()\n",
    "\n",
    "# # 创建预测结果DataFrame\n",
    "# predictions_df = pd.DataFrame({\n",
    "#     'Id': range(len(test_predictions)),\n",
    "#     'Predicted': test_predictions.flatten()\n",
    "# })\n",
    "\n",
    "# # 保存预测结果\n",
    "# predictions_df.to_csv('data/nn_test.csv', index=False)\n",
    "# print(\"预测结果已保存到 data/nn_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
