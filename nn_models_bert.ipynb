{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集读取完成\n",
      "训练集大小: (20000, 74)\n",
      "验证集大小: (5000, 74)\n",
      "全量训练集大小: (25000, 74)\n",
      "测试集大小: (10000, 74)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import json\n",
    "import math\n",
    "\n",
    "# 读取处理后的数据集\n",
    "X_train = pd.read_csv('data/processed/X_train.csv')\n",
    "y_train = pd.read_csv('data/processed/y_train.csv')\n",
    "X_valid = pd.read_csv('data/processed/X_valid.csv')\n",
    "y_valid = pd.read_csv('data/processed/y_valid.csv') \n",
    "X_train_full = pd.read_csv('data/processed/X_train_full.csv') \n",
    "y_train_full = pd.read_csv('data/processed/y_train_full.csv') \n",
    "X_test = pd.read_csv('data/processed/X_test.csv')\n",
    "\n",
    "print('数据集读取完成')\n",
    "print(f'训练集大小: {X_train.shape}')\n",
    "print(f'验证集大小: {X_valid.shape}')\n",
    "print(f'全量训练集大小: {X_train_full.shape}')\n",
    "print(f'测试集大小: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: (20000, 55)\n",
      "全量集: (25000, 55)\n",
      "测试集: (10000, 55)\n"
     ]
    }
   ],
   "source": [
    "# 直接定义列名配置\n",
    "del_cols = ['listing_id', 'original_reg_date', 'opc_scheme', 'lifespan', 'eco_category', 'indicative_price']\n",
    "text_cols = ['title', 'description', 'features', 'accessories']\n",
    "date_cols = ['reg_date']\n",
    "numeric_cols = ['manufactured', 'curb_weight', 'power', 'engine_cap', 'depreciation', 'coe', 'road_tax', \n",
    "                'dereg_value', 'mileage', 'omv', 'arf', 'year', 'month',\n",
    "                'text_brand_popularity_score', 'text_model_value_score', 'text_condition_score',\n",
    "                'text_feature_rarity_score', 'text_performance_score', 'text_sentiment_score']\n",
    "log_cols = ['manufactured', 'curb_weight', 'power_log', 'engine_cap_log', 'depreciation_log', 'coe', \n",
    "            'road_tax_log', 'dereg_value_log', 'mileage_log', 'omv_log', 'arf_log', 'year', 'month']\n",
    "root_cols = ['manufactured', 'curb_weight', 'power_root', 'engine_cap_root', 'depreciation_root', 'coe', \n",
    "             'road_tax_root', 'dereg_value_root', 'mileage_root', 'omv_root', 'arf_root', 'year', 'month']\n",
    "categorical_cols = ['make', 'model', 'type_of_vehicle', 'category', 'transmission', 'fuel_type', 'no_of_owners']\n",
    "\n",
    "# 更新变换列，添加GPT特征\n",
    "cat_nu_cols = [\n",
    "    \"manufactured\", \"curb_weight\", \"power\", \"engine_cap\", \"no_of_owners\", \"depreciation\", \n",
    "    \"coe\", \"road_tax\", \"dereg_value\", \"mileage\", \"omv\", \"arf\", \"make_target_encoded\",\n",
    "    \"text_brand_popularity_score\", \"text_model_value_score\", \"text_condition_score\",\n",
    "    \"text_feature_rarity_score\", \"text_performance_score\", \"text_sentiment_score\",\n",
    "    \"-\", \"almost new car\", \"coe car\", \"consignment car\", \"direct owner sale\", \n",
    "    \"electric cars\", \"hybrid cars\", \"imported used vehicle\", \"low mileage car\", \n",
    "    \"opc car\", \"parf car\", \"premium ad car\", \"rare & exotic\", \"sgcarmart warranty cars\", \n",
    "    \"sta evaluated car\", \"vintage cars\", \"type_of_vehicle_bus/mini bus\", \n",
    "    \"type_of_vehicle_hatchback\", \"type_of_vehicle_luxury sedan\", \n",
    "    \"type_of_vehicle_mid-sized sedan\", \"type_of_vehicle_mpv\", \"type_of_vehicle_others\", \n",
    "    \"type_of_vehicle_sports car\", \"type_of_vehicle_stationwagon\", \"type_of_vehicle_suv\", \n",
    "    \"type_of_vehicle_truck\", \"type_of_vehicle_van\", \"fuel_type_diesel\", \n",
    "    \"fuel_type_diesel-electric\", \"fuel_type_electric\", \"fuel_type_petrol\", \n",
    "    \"fuel_type_petrol-electric\", \"fuel_type_nan\", \"transmission_manual\", \"year\", \"month\"\n",
    "]\n",
    "\n",
    "cat_log_cols = [\n",
    "    \"manufactured\", \"curb_weight\", \"power_log\", \"engine_cap_log\", \"depreciation_log\", \n",
    "    \"coe\", \"road_tax_log\", \"dereg_value_log\", \"mileage_log\", \"omv_log\", \"arf_log\", \n",
    "    \"make_target_encoded\", \"text_brand_popularity_score\", \"text_model_value_score\", \n",
    "    \"text_condition_score\", \"text_feature_rarity_score\", \"text_performance_score\", \n",
    "    \"text_sentiment_score\", \"-\", \"almost new car\", \"coe car\", \"consignment car\", \n",
    "    \"direct owner sale\", \"electric cars\", \"hybrid cars\", \"imported used vehicle\", \n",
    "    \"low mileage car\", \"opc car\", \"parf car\", \"premium ad car\", \"rare & exotic\", \n",
    "    \"sgcarmart warranty cars\", \"sta evaluated car\", \"vintage cars\", \n",
    "    \"type_of_vehicle_bus/mini bus\", \"type_of_vehicle_hatchback\", \n",
    "    \"type_of_vehicle_luxury sedan\", \"type_of_vehicle_mid-sized sedan\", \n",
    "    \"type_of_vehicle_mpv\", \"type_of_vehicle_others\", \"type_of_vehicle_sports car\", \n",
    "    \"type_of_vehicle_stationwagon\", \"type_of_vehicle_suv\", \"type_of_vehicle_truck\", \n",
    "    \"type_of_vehicle_van\", \"fuel_type_diesel\", \"fuel_type_diesel-electric\", \n",
    "    \"fuel_type_electric\", \"fuel_type_petrol\", \"fuel_type_petrol-electric\", \n",
    "    \"fuel_type_nan\", \"transmission_manual\", \"year\", \"month\"\n",
    "]\n",
    "\n",
    "cat_root_cols = [\n",
    "    \"manufactured\", \"curb_weight\", \"power_root\", \"engine_cap_root\", \"depreciation_root\", \n",
    "    \"coe\", \"road_tax_root\", \"dereg_value_root\", \"mileage_root\", \"omv_root\", \"arf_root\", \n",
    "    \"make_target_encoded\", \"text_brand_popularity_score\", \"text_model_value_score\", \n",
    "    \"text_condition_score\", \"text_feature_rarity_score\", \"text_performance_score\", \n",
    "    \"text_sentiment_score\", \"-\", \"almost new car\", \"coe car\", \"consignment car\", \n",
    "    \"direct owner sale\", \"electric cars\", \"hybrid cars\", \"imported used vehicle\", \n",
    "    \"low mileage car\", \"opc car\", \"parf car\", \"premium ad car\", \"rare & exotic\", \n",
    "    \"sgcarmart warranty cars\", \"sta evaluated car\", \"vintage cars\", \n",
    "    \"type_of_vehicle_bus/mini bus\", \"type_of_vehicle_hatchback\", \n",
    "    \"type_of_vehicle_luxury sedan\", \"type_of_vehicle_mid-sized sedan\", \n",
    "    \"type_of_vehicle_mpv\", \"type_of_vehicle_others\", \"type_of_vehicle_sports car\", \n",
    "    \"type_of_vehicle_stationwagon\", \"type_of_vehicle_suv\", \"type_of_vehicle_truck\", \n",
    "    \"type_of_vehicle_van\", \"fuel_type_diesel\", \"fuel_type_diesel-electric\", \n",
    "    \"fuel_type_electric\", \"fuel_type_petrol\", \"fuel_type_petrol-electric\", \n",
    "    \"fuel_type_nan\", \"transmission_manual\", \"year\", \"month\"\n",
    "]\n",
    "\n",
    "# 丢弃log和root变换的结果\n",
    "X_train = X_train[cat_nu_cols]\n",
    "X_valid = X_valid[cat_nu_cols]\n",
    "X_test = X_test[cat_nu_cols]\n",
    "X_train_full = X_train_full[cat_nu_cols]\n",
    "\n",
    "print(f'训练集: {X_train.shape}')\n",
    "print(f'全量集: {X_train_full.shape}')\n",
    "print(f'测试集: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\10021\\anaconda3\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征维度:\n",
      "原始特征: 55\n",
      "UMAP特征: 8\n",
      "组合特征: 63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from umap.umap_ import UMAP  # 正确的导入方式\n",
    "\n",
    "# 加载BERT向量\n",
    "bert_train_vectors = np.load('data/processed/train_vectors.npy')\n",
    "bert_valid_vectors = np.load('data/processed/valid_vectors.npy')\n",
    "bert_train_full_vectors = np.load('data/processed/train_full_vectors.npy')\n",
    "bert_test_vectors = np.load('data/processed/test_vectors.npy')\n",
    "\n",
    "# BERT降维\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# UMAP参数设置\n",
    "umap = UMAP(\n",
    "    n_components=8,\n",
    "    n_neighbors=20,\n",
    "    min_dist=0.1,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# umap = UMAP(\n",
    "#     n_components=16,\n",
    "#     n_neighbors=30,\n",
    "#     min_dist=0.3,\n",
    "#     metric='cosine',\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# umap = UMAP(\n",
    "#     n_components=24,\n",
    "#     n_neighbors=50,\n",
    "#     min_dist=0.5,\n",
    "#     metric='cosine',\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# 对BERT向量进行UMAP降维\n",
    "bert_train_scaled = scaler.fit_transform(bert_train_vectors)\n",
    "bert_train_umap = umap.fit_transform(bert_train_scaled)\n",
    "\n",
    "# 对验证集和测试集应用相同的转换\n",
    "bert_valid_scaled = scaler.transform(bert_valid_vectors)\n",
    "bert_valid_umap = umap.transform(bert_valid_scaled)\n",
    "\n",
    "bert_test_scaled = scaler.transform(bert_test_vectors)\n",
    "bert_test_umap = umap.transform(bert_test_scaled)\n",
    "\n",
    "# 对完整训练集进行转换\n",
    "bert_train_full_scaled = scaler.transform(bert_train_full_vectors)\n",
    "bert_train_full_umap = umap.transform(bert_train_full_scaled)\n",
    "\n",
    "# 拼接特征\n",
    "X_train_combined = np.hstack((X_train[cat_nu_cols].values, bert_train_umap))\n",
    "X_valid_combined = np.hstack((X_valid[cat_nu_cols].values, bert_valid_umap))\n",
    "X_test_combined = np.hstack((X_test[cat_nu_cols].values, bert_test_umap))\n",
    "X_train_full_combined = np.hstack((X_train_full[cat_nu_cols].values, bert_train_full_umap))\n",
    "\n",
    "\n",
    "# 打印维度信息\n",
    "print(\"特征维度:\")\n",
    "print(f\"原始特征: {X_train[cat_nu_cols].shape[1]}\")\n",
    "print(f\"UMAP特征: {bert_train_umap.shape[1]}\")\n",
    "print(f\"组合特征: {X_train_combined.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 63)\n",
      "(10000, 63)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_full_combined.shape)\n",
    "print(X_test_combined.shape)\n",
    "X_train = X_train_combined\n",
    "X_valid = X_valid_combined\n",
    "X_test = X_test_combined\n",
    "X_train_full = X_train_full_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, lr=0.001, wd=0):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "    y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    epochs = 50\n",
    "    best_rmse = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test_pred_mlp = model(X_valid_tensor)\n",
    "            mse_test_mlp = mean_squared_error(y_valid_tensor.numpy(), y_test_pred_mlp.numpy())\n",
    "            rmse_test_mlp = np.sqrt(mse_test_mlp)\n",
    "\n",
    "        # 更新最佳RMSE\n",
    "        if rmse_test_mlp < best_rmse:\n",
    "            best_rmse = rmse_test_mlp\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # 训练流程结束后打印最佳RMSE\n",
    "    print(f'Best Valid RMSE: {best_rmse:.4f}')\n",
    "\n",
    "input_dim = X_train_combined.shape[1]\n",
    "print(input_dim)\n",
    "        \n",
    "# input_dim = len(cat_nu_cols)\n",
    "# print(input_dim)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Valid RMSE: 29849.7988\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 24896.9609\n",
      "Best Valid RMSE: 29919.1641\n",
      "Best Valid RMSE: 42777.7188\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = MLPModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.1)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.01)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.0001)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MLPModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0)\n",
    "# model = MLPModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.0001)\n",
    "# model = MLPModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.001)\n",
    "# model = MLPModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.01)\n",
    "# model = MLPModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPWithBertModel(nn.Module):\n",
    "    def __init__(self, input_dim, bert_dim=8, reduced_dim=16):\n",
    "        super(MLPWithBertModel, self).__init__()\n",
    "        # Assuming the BERT output is at the last part of the input\n",
    "        self.bert_processor = nn.Sequential(\n",
    "            nn.Linear(bert_dim, reduced_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # New input dimension after concatenating reduced BERT output\n",
    "        new_input_dim = input_dim - bert_dim + reduced_dim\n",
    "        \n",
    "        self.layer1 = nn.Linear(new_input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Assuming x is the input where the last 1024 elements are the BERT vector\n",
    "        bert_vector = x[:, -8:]  # Extract the last 1024 dimensions\n",
    "        other_features = x[:, :-8]  # Extract all other features\n",
    "        \n",
    "        # Process the BERT vector\n",
    "        processed_bert = self.bert_processor(bert_vector)\n",
    "        \n",
    "        # Concatenate the processed BERT output with other features\n",
    "        x = torch.cat((other_features, processed_bert), dim=1)\n",
    "        \n",
    "        # Feed through the subsequent layers\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 30926.9043\n",
      "Best Valid RMSE: 24532.4688\n",
      "Best Valid RMSE: 28926.2031\n",
      "Best Valid RMSE: 43807.6602\n"
     ]
    }
   ],
   "source": [
    "model = MLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.01)\n",
    "model = MLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001)\n",
    "model = MLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.0001)\n",
    "model = MLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Valid RMSE: 28641.8184\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 25722.6543\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m MLPModel(input_dim)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_nu_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 21\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, lr, wd)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 21\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[19], line 23\u001b[0m, in \u001b[0;36mMLPModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x))\n\u001b[0;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x))\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDropoutModel(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.5):\n",
    "        super(MLPDropoutModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPDropoutModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = MLPDropoutModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = MLPDropoutModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = MLPDropoutModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.0001)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.001)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.01)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperMLPModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeeperMLPModel, self).__init__()\n",
    "        # Increasing the depth with more layers\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.layer3 = nn.Linear(128, 128)\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.layer5 = nn.Linear(64, 64)\n",
    "        self.layer6 = nn.Linear(64, 32)\n",
    "        self.layer7 = nn.Linear(32, 16)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights with Kaiming initialization suitable for ReLU\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.relu(self.layer5(x))\n",
    "        x = self.relu(self.layer6(x))\n",
    "        x = self.relu(self.layer7(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        # Apply Kaiming He initialization to all linear layers in the model\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeeperMLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.00001)\n",
    "model = DeeperMLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = DeeperMLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = DeeperMLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperMLPWithBertModel(nn.Module):\n",
    "    def __init__(self, input_dim, bert_dim=8, reduced_bert_dim=128):\n",
    "        super(DeeperMLPWithBertModel, self).__init__()\n",
    "        \n",
    "        # Processing the BERT vector\n",
    "        self.bert_processor = nn.Sequential(\n",
    "            nn.Linear(bert_dim, reduced_bert_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # New input dimension after reducing the BERT vector and concatenating it back\n",
    "        new_input_dim = input_dim - bert_dim + reduced_bert_dim\n",
    "\n",
    "        # Increasing the depth with more layers\n",
    "        self.layer1 = nn.Linear(new_input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.layer3 = nn.Linear(128, 128)\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.layer5 = nn.Linear(64, 64)\n",
    "        self.layer6 = nn.Linear(64, 32)\n",
    "        self.layer7 = nn.Linear(32, 16)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights with Kaiming initialization suitable for ReLU\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split the input into BERT vector and other features\n",
    "        bert_vector = x[:, -8:]  # Assuming BERT vector is the last 1024 elements\n",
    "        other_features = x[:, :-8]  # The rest of the features\n",
    "\n",
    "        # Process the BERT vector\n",
    "        processed_bert = self.bert_processor(bert_vector)\n",
    "\n",
    "        # Concatenate the processed BERT vector with other features\n",
    "        x = torch.cat((other_features, processed_bert), dim=1)\n",
    "\n",
    "        # Sequentially process through all layers\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.relu(self.layer5(x))\n",
    "        x = self.relu(self.layer6(x))\n",
    "        x = self.relu(self.layer7(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeeperMLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.00001)\n",
    "model = DeeperMLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = DeeperMLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = DeeperMLPWithBertModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepResidualMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepResidualMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, 256)\n",
    "        self.layer4 = nn.Linear(256, 256)\n",
    "        self.layer5 = nn.Linear(256, 256)\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights with He initialization suitable for ReLU\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.relu(self.layer1(x))\n",
    "        \n",
    "        # Residual block 1\n",
    "        out = self.relu(self.layer2(x))\n",
    "        x = out + x  # Changed from 'out += x' to 'x = out + x'\n",
    "    \n",
    "        # Residual block 2\n",
    "        out = self.relu(self.layer3(x))\n",
    "        x = out + x  # Changed from 'x += out' to 'x = out + x'\n",
    "    \n",
    "        # Residual block 3\n",
    "        out = self.relu(self.layer4(x))\n",
    "        x = out + x  # Changed from 'out += x' to 'x = out + x'\n",
    "    \n",
    "        # Residual block 4\n",
    "        out = self.relu(self.layer5(x))\n",
    "        x = out + x  # Changed from 'x += out' to 'x = out + x'\n",
    "    \n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.1)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepResidualMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepResidualMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, 256)\n",
    "        self.layer4 = nn.Linear(256, 256)\n",
    "        self.layer5 = nn.Linear(256, 256)\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights with He initialization suitable for ReLU\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input pass\n",
    "        identity = x\n",
    "        x = self.relu(self.layer1(x))\n",
    "        \n",
    "        # Residual block 1\n",
    "        out = self.relu(self.layer2(x))\n",
    "        out += x  # Adding input after the block\n",
    "        \n",
    "        # Residual block 2\n",
    "        x = self.relu(self.layer3(out))\n",
    "        x += out  # Adding input from the previous block\n",
    "        \n",
    "        # Residual block 3\n",
    "        out = self.relu(self.layer4(x))\n",
    "        out += x  # Adding input from the previous block\n",
    "        \n",
    "        # Residual block 4\n",
    "        x = self.relu(self.layer5(out))\n",
    "        x += out  # Adding input from the previous block\n",
    "\n",
    "        # Output pass\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        self.key_layer = nn.Linear(feature_dim, feature_dim, bias=False)\n",
    "        self.query_layer = nn.Linear(feature_dim, feature_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query_layer(x).unsqueeze(1)  # Adding batch dimension\n",
    "        key = self.key_layer(x).unsqueeze(-1)  # Adding an extra dimension for bmm\n",
    "        \n",
    "        # Compute attention scores and apply softmax\n",
    "        scores = torch.bmm(query, key)  # Should work as both are 3D now\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply weights to the original input features, using batch matrix multiplication\n",
    "        attended = torch.bmm(weights, x.unsqueeze(1))  # x also needs to be 3D\n",
    "        return attended.squeeze(1)  # Remove the extra dimension to match expected output shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeepResidualMLPWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepResidualMLPWithAttention, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, 256)\n",
    "        self.layer4 = nn.Linear(256, 256)\n",
    "        self.layer5 = nn.Linear(256, 256)\n",
    "        self.attention = Attention(256)  # Attention layer after layer5\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.relu(self.layer1(x))\n",
    "        \n",
    "        out = self.relu(self.layer2(x))\n",
    "        out = out + x  # Use out-of-place operation\n",
    "\n",
    "        x = self.relu(self.layer3(out))\n",
    "        x = x + out  # Use out-of-place operation\n",
    "\n",
    "        out = self.relu(self.layer4(x))\n",
    "        out = out + x  # Use out-of-place operation\n",
    "\n",
    "        x = self.relu(self.layer5(out))\n",
    "        x = x + out  # Use out-of-place operation\n",
    "\n",
    "        # Apply attention\n",
    "        x = self.attention(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepResidualMLPWithAttention(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = DeepResidualMLPWithAttention(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = DeepResidualMLPWithAttention(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = DeepResidualMLPWithAttention(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network_and_get_result(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, X_test, lr=0.001, wd=0):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "    y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    epochs = 50\n",
    "    best_rmse = float('inf')\n",
    "    best_model = None\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_valid_pred = model(X_valid_tensor).numpy().flatten()\n",
    "            mse_valid = mean_squared_error(y_valid_tensor.numpy(), y_valid_pred)\n",
    "            rmse_valid = np.sqrt(mse_valid)\n",
    "\n",
    "        if rmse_valid < best_rmse:\n",
    "            best_rmse = rmse_valid\n",
    "            best_model = model\n",
    "            torch.save(best_model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(f'Best Valid RMSE: {best_rmse:.4f}')\n",
    "\n",
    "    # Saving validation predictions to a CSV file\n",
    "    valid_predictions_df = pd.DataFrame({\n",
    "        'Id': range(len(y_valid_pred)),\n",
    "        'Predicted': y_valid_pred\n",
    "    })\n",
    "    valid_predictions_df.to_csv('data/nn_valid.csv', index=False)\n",
    "    print(\"Validation results saved to data/nn_valid.csv\")\n",
    "\n",
    "    # Saving test predictions to a CSV file if the best model was found\n",
    "    if best_model:\n",
    "        best_model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test_pred = best_model(X_test_tensor).numpy().flatten()\n",
    "\n",
    "        test_predictions_df = pd.DataFrame({\n",
    "            'Id': range(len(y_test_pred)),\n",
    "            'Predicted': y_test_pred\n",
    "        })\n",
    "        test_predictions_df.to_csv('data/nn_test.csv', index=False)\n",
    "        print(\"Test results saved to data/nn_test.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Valid RMSE: 25162.6660\n",
      "Validation results saved to data/nn_valid.csv\n",
      "Test results saved to data/nn_test.csv\n"
     ]
    }
   ],
   "source": [
    "model = MLPWithBertModel(input_dim)\n",
    "train_network_and_get_result(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, X_test,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 选择表现最好的模型进行最终训练和预测\n",
    "# best_model = DeeperMLPModel(input_dim)\n",
    "\n",
    "# print(\"使用全量数据训练最终模型...\")\n",
    "# # 转换数据为tensor\n",
    "# X_train_full_tensor = torch.tensor(X_train_full[cat_nu_cols].values, dtype=torch.float32)\n",
    "# y_train_full_tensor = torch.tensor(y_train_full.values, dtype=torch.float32).view(-1, 1)\n",
    "# train_dataset = TensorDataset(X_train_full_tensor, y_train_full_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# # 训练模型\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
    "\n",
    "# epochs = 50\n",
    "# for epoch in range(epochs):\n",
    "#     best_model.train()\n",
    "#     for inputs, targets in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = best_model(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# print(\"生成测试集预测结果...\")\n",
    "# best_model.eval()\n",
    "# X_test_tensor = torch.tensor(X_test[cat_nu_cols].values, dtype=torch.float32)\n",
    "# with torch.no_grad():\n",
    "#     test_predictions = best_model(X_test_tensor).numpy()\n",
    "\n",
    "# # 创建预测结果DataFrame\n",
    "# predictions_df = pd.DataFrame({\n",
    "#     'Id': range(len(test_predictions)),\n",
    "#     'Predicted': test_predictions.flatten()\n",
    "# })\n",
    "\n",
    "# # 保存预测结果\n",
    "# predictions_df.to_csv('data/nn_test.csv', index=False)\n",
    "# print(\"预测结果已保存到 data/nn_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
