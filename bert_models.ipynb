{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集读取完成\n",
      "训练集大小: (20000, 1092)\n",
      "验证集大小: (5000, 1092)\n",
      "全量训练集大小: (25000, 1092)\n",
      "测试集大小: (10000, 1092)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Function to load vectors and merge them into the existing DataFrame\n",
    "def load_and_merge_vectors(df, vector_file):\n",
    "    # Load the BERT vectors from the .npy file\n",
    "    vectors = np.load(vector_file)\n",
    "    \n",
    "    # Convert numpy array to DataFrame\n",
    "    vector_df = pd.DataFrame(vectors, columns=[f'bert_vector_{i}' for i in range(vectors.shape[1])])\n",
    "    \n",
    "    # Ensure the indices match before concatenation if the DataFrame is not empty\n",
    "    if not df.index.equals(vector_df.index):\n",
    "        vector_df.index = df.index\n",
    "    \n",
    "    # Concatenate the vector DataFrame with the original DataFrame\n",
    "    return pd.concat([df, vector_df], axis=1)\n",
    "\n",
    "# Load datasets\n",
    "X_train = pd.read_csv('data/processed/X_train.csv')\n",
    "y_train = pd.read_csv('data/processed/y_train.csv')\n",
    "X_valid = pd.read_csv('data/processed/X_valid.csv')\n",
    "y_valid = pd.read_csv('data/processed/y_valid.csv')\n",
    "X_train_full = pd.read_csv('data/processed/X_train_full.csv')\n",
    "y_train_full = pd.read_csv('data/processed/y_train_full.csv')\n",
    "X_test = pd.read_csv('data/processed/X_test.csv')\n",
    "\n",
    "# Load and merge BERT vectors\n",
    "X_train = load_and_merge_vectors(X_train, 'data/processed/train_vectors.npy')\n",
    "X_valid = load_and_merge_vectors(X_valid, 'data/processed/valid_vectors.npy')\n",
    "X_train_full = load_and_merge_vectors(X_train_full, 'data/processed/train_full_vectors.npy')\n",
    "X_test = load_and_merge_vectors(X_test, 'data/processed/test_vectors.npy')\n",
    "\n",
    "print('数据集读取完成')\n",
    "print(f'训练集大小: {X_train.shape}')\n",
    "print(f'验证集大小: {X_valid.shape}')\n",
    "print(f'全量训练集大小: {X_train_full.shape}')\n",
    "print(f'测试集大小: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从JSON文件读取列名配置\n",
    "with open('./data/columns.json', 'r') as f:\n",
    "    columns_dict = json.load(f)\n",
    "\n",
    "# 从字典中读取列名\n",
    "del_cols = columns_dict['del_cols']\n",
    "text_cols = columns_dict['text_cols'] \n",
    "date_cols = columns_dict['date_cols']\n",
    "numeric_cols = columns_dict['numeric_cols']\n",
    "log_cols = columns_dict['log_cols']\n",
    "root_cols = columns_dict['root_cols']\n",
    "categorical_cols = columns_dict['categorical_cols']\n",
    "\n",
    "# 定义变换列\n",
    "cat_nu_cols = [\"manufactured\", \"curb_weight\", \"power\", \"engine_cap\", \"no_of_owners\", \"depreciation\", \"coe\", \"road_tax\", \"dereg_value\", \"mileage\", \"omv\", \"arf\", \"make_target_encoded\", \"-\", \"almost new car\", \"coe car\", \"consignment car\", \"direct owner sale\", \"electric cars\", \"hybrid cars\", \"imported used vehicle\", \"low mileage car\", \"opc car\", \"parf car\", \"premium ad car\", \"rare & exotic\", \"sgcarmart warranty cars\", \"sta evaluated car\", \"vintage cars\", \"type_of_vehicle_bus/mini bus\", \"type_of_vehicle_hatchback\", \"type_of_vehicle_luxury sedan\", \"type_of_vehicle_mid-sized sedan\", \"type_of_vehicle_mpv\", \"type_of_vehicle_others\", \"type_of_vehicle_sports car\", \"type_of_vehicle_stationwagon\", \"type_of_vehicle_suv\", \"type_of_vehicle_truck\", \"type_of_vehicle_van\", \"fuel_type_diesel\", \"fuel_type_diesel-electric\", \"fuel_type_electric\", \"fuel_type_petrol\", \"fuel_type_petrol-electric\", \"fuel_type_nan\", \"transmission_manual\", \"year\", \"month\"]\n",
    "cat_log_cols = [\"manufactured\", \"curb_weight\", \"power_log\", \"engine_cap_log\", \"depreciation_log\", \"coe\", \"road_tax_log\", \"dereg_value_log\", \"mileage_log\", \"omv_log\", \"arf_log\", \"make_target_encoded\", \"-\", \"almost new car\", \"coe car\", \"consignment car\", \"direct owner sale\", \"electric cars\", \"hybrid cars\", \"imported used vehicle\", \"low mileage car\", \"opc car\", \"parf car\", \"premium ad car\", \"rare & exotic\", \"sgcarmart warranty cars\", \"sta evaluated car\", \"vintage cars\", \"type_of_vehicle_bus/mini bus\", \"type_of_vehicle_hatchback\", \"type_of_vehicle_luxury sedan\", \"type_of_vehicle_mid-sized sedan\", \"type_of_vehicle_mpv\", \"type_of_vehicle_others\", \"type_of_vehicle_sports car\", \"type_of_vehicle_stationwagon\", \"type_of_vehicle_suv\", \"type_of_vehicle_truck\", \"type_of_vehicle_van\", \"fuel_type_diesel\", \"fuel_type_diesel-electric\", \"fuel_type_electric\", \"fuel_type_petrol\", \"fuel_type_petrol-electric\", \"fuel_type_nan\", \"transmission_manual\", \"year\", \"month\"]\n",
    "cat_root_cols = [\"manufactured\", \"curb_weight\", \"power_root\", \"engine_cap_root\", \"depreciation_root\", \"coe\", \"road_tax_root\", \"dereg_value_root\", \"mileage_root\", \"omv_root\", \"arf_root\", \"make_target_encoded\", \"-\", \"almost new car\", \"coe car\", \"consignment car\", \"direct owner sale\", \"electric cars\", \"hybrid cars\", \"imported used vehicle\", \"low mileage car\", \"opc car\", \"parf car\", \"premium ad car\", \"rare & exotic\", \"sgcarmart warranty cars\", \"sta evaluated car\", \"vintage cars\", \"type_of_vehicle_bus/mini bus\", \"type_of_vehicle_hatchback\", \"type_of_vehicle_luxury sedan\", \"type_of_vehicle_mid-sized sedan\", \"type_of_vehicle_mpv\", \"type_of_vehicle_others\", \"type_of_vehicle_sports car\", \"type_of_vehicle_stationwagon\", \"type_of_vehicle_suv\", \"type_of_vehicle_truck\", \"type_of_vehicle_van\", \"fuel_type_diesel\", \"fuel_type_diesel-electric\", \"fuel_type_electric\", \"fuel_type_petrol\", \"fuel_type_petrol-electric\", \"fuel_type_nan\", \"transmission_manual\", \"year\", \"month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "len_pca = 20\n",
    "\n",
    "X_train = X_train[cat_nu_cols].values\n",
    "X_valid = X_valid[cat_nu_cols].values\n",
    "X_train_full = X_train_full[cat_nu_cols].values\n",
    "\n",
    "# 标准化训练数据\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # 训练集用 fit_transform\n",
    "\n",
    "# 初始化 PCA，这里假设我们降维到10维\n",
    "pca = PCA(n_components=len_pca)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)  # 训练集用 fit_transform\n",
    "\n",
    "# 对验证数据和完整训练数据应用相同的标准化和 PCA 转换\n",
    "X_valid_scaled = scaler.transform(X_valid)  # 验证集和完整训练集仅用 transform\n",
    "X_valid_pca = pca.transform(X_valid_scaled)\n",
    "\n",
    "X_train_full_scaled = scaler.transform(X_train_full)\n",
    "X_train_full_pca = pca.transform(X_train_full_scaled)\n",
    "\n",
    "X_train = np.hstack((X_train, X_train_pca))\n",
    "X_valid = np.hstack((X_valid, X_valid_pca))\n",
    "X_train_full = np.hstack((X_train_full, X_train_full_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, lr=0.001, wd=0):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "    y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    epochs = 50\n",
    "    best_rmse = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_test_pred_mlp = model(X_valid_tensor)\n",
    "            mse_test_mlp = mean_squared_error(y_valid_tensor.numpy(), y_test_pred_mlp.numpy())\n",
    "            rmse_test_mlp = np.sqrt(mse_test_mlp)\n",
    "\n",
    "        # 更新最佳RMSE\n",
    "        if rmse_test_mlp < best_rmse:\n",
    "            best_rmse = rmse_test_mlp\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # 训练流程结束后打印最佳RMSE\n",
    "    print(f'Best Valid RMSE: {best_rmse:.4f}')\n",
    "\n",
    "input_dim = len_pca+len(cat_nu_cols)\n",
    "print(input_dim)\n",
    "        \n",
    "# input_dim = len(cat_nu_cols)\n",
    "# print(input_dim)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n",
      "Best Valid RMSE: 30877.2188\n",
      "Best Valid RMSE: 26305.2793\n",
      "Best Valid RMSE: 28921.2363\n",
      "Best Valid RMSE: 43425.2500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = MLPModel(input_dim)\n",
    "# train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.1)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.01)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.0001)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.0001)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.001)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.01)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Valid RMSE: 29270.1504\n",
      "Best Valid RMSE: 25254.7285\n",
      "Best Valid RMSE: 29316.7891\n"
     ]
    }
   ],
   "source": [
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = MLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDropoutModel(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.5):\n",
    "        super(MLPDropoutModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Valid RMSE: 40491.0469\n",
      "Best Valid RMSE: 44801.4727\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 41970.9805\n",
      "Best Valid RMSE: 44968.7578\n"
     ]
    }
   ],
   "source": [
    "model = MLPDropoutModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = MLPDropoutModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = MLPDropoutModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = MLPDropoutModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Valid RMSE: 30573.6875\n",
      "Best Valid RMSE: 35608.0938\n",
      "Best Valid RMSE: 30325.9961\n",
      "Best Valid RMSE: 36972.2383\n"
     ]
    }
   ],
   "source": [
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Valid RMSE: 30277.0566\n",
      "Best Valid RMSE: 30836.6738\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 31781.8320\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 31050.0410\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 31057.7891\n"
     ]
    }
   ],
   "source": [
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.0001)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.001)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.01)\n",
    "model = MLPDropoutModel(input_dim, 0.1)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid,0.001,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperMLPModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeeperMLPModel, self).__init__()\n",
    "        # Increasing the depth with more layers\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.layer3 = nn.Linear(128, 128)\n",
    "        self.layer4 = nn.Linear(128, 64)\n",
    "        self.layer5 = nn.Linear(64, 64)\n",
    "        self.layer6 = nn.Linear(64, 32)\n",
    "        self.layer7 = nn.Linear(32, 16)\n",
    "        self.output_layer = nn.Linear(16, 1)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights with Kaiming initialization suitable for ReLU\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.relu(self.layer5(x))\n",
    "        x = self.relu(self.layer6(x))\n",
    "        x = self.relu(self.layer7(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        # Apply Kaiming He initialization to all linear layers in the model\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Valid RMSE: 32396.6328\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 27172.0742\n",
      "Best Valid RMSE: 25258.3008\n",
      "Early stopping triggered.\n",
      "Best Valid RMSE: 26628.1309\n"
     ]
    }
   ],
   "source": [
    "model = DeeperMLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.00001)\n",
    "model = DeeperMLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = DeeperMLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = DeeperMLPModel(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepResidualMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepResidualMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, 256)\n",
    "        self.layer4 = nn.Linear(256, 256)\n",
    "        self.layer5 = nn.Linear(256, 256)\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights with He initialization suitable for ReLU\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.relu(self.layer1(x))\n",
    "        \n",
    "        # Residual block 1\n",
    "        out = self.relu(self.layer2(x))\n",
    "        x = out + x  # Changed from 'out += x' to 'x = out + x'\n",
    "    \n",
    "        # Residual block 2\n",
    "        out = self.relu(self.layer3(x))\n",
    "        x = out + x  # Changed from 'x += out' to 'x = out + x'\n",
    "    \n",
    "        # Residual block 3\n",
    "        out = self.relu(self.layer4(x))\n",
    "        x = out + x  # Changed from 'out += x' to 'x = out + x'\n",
    "    \n",
    "        # Residual block 4\n",
    "        out = self.relu(self.layer5(x))\n",
    "        x = out + x  # Changed from 'x += out' to 'x = out + x'\n",
    "    \n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Valid RMSE: 23946.3398\n",
      "Best Valid RMSE: 26799.8789\n",
      "Best Valid RMSE: 24532.9941\n",
      "Best Valid RMSE: 26980.0684\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.1)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 256]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m     44\u001b[0m model \u001b[38;5;241m=\u001b[39m DeepResidualMLP(input_dim)\n\u001b[1;32m---> 45\u001b[0m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_nu_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, lr, wd)\u001b[0m\n\u001b[0;32m     21\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\10021\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [32, 256]], which is output 0 of ReluBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "class DeepResidualMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepResidualMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, 256)\n",
    "        self.layer4 = nn.Linear(256, 256)\n",
    "        self.layer5 = nn.Linear(256, 256)\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights with He initialization suitable for ReLU\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input pass\n",
    "        identity = x\n",
    "        x = self.relu(self.layer1(x))\n",
    "        \n",
    "        # Residual block 1\n",
    "        out = self.relu(self.layer2(x))\n",
    "        out += x  # Adding input after the block\n",
    "        \n",
    "        # Residual block 2\n",
    "        x = self.relu(self.layer3(out))\n",
    "        x += out  # Adding input from the previous block\n",
    "        \n",
    "        # Residual block 3\n",
    "        out = self.relu(self.layer4(x))\n",
    "        out += x  # Adding input from the previous block\n",
    "        \n",
    "        # Residual block 4\n",
    "        x = self.relu(self.layer5(out))\n",
    "        x += out  # Adding input from the previous block\n",
    "\n",
    "        # Output pass\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = DeepResidualMLP(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        self.key_layer = nn.Linear(feature_dim, feature_dim, bias=False)\n",
    "        self.query_layer = nn.Linear(feature_dim, feature_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.query_layer(x).unsqueeze(1)  # Adding batch dimension\n",
    "        key = self.key_layer(x).unsqueeze(-1)  # Adding an extra dimension for bmm\n",
    "        \n",
    "        # Compute attention scores and apply softmax\n",
    "        scores = torch.bmm(query, key)  # Should work as both are 3D now\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply weights to the original input features, using batch matrix multiplication\n",
    "        attended = torch.bmm(weights, x.unsqueeze(1))  # x also needs to be 3D\n",
    "        return attended.squeeze(1)  # Remove the extra dimension to match expected output shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DeepResidualMLPWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DeepResidualMLPWithAttention, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, 256)\n",
    "        self.layer4 = nn.Linear(256, 256)\n",
    "        self.layer5 = nn.Linear(256, 256)\n",
    "        self.attention = Attention(256)  # Attention layer after layer5\n",
    "        self.output_layer = nn.Linear(256, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initializing weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.relu(self.layer1(x))\n",
    "        \n",
    "        out = self.relu(self.layer2(x))\n",
    "        out = out + x  # Use out-of-place operation\n",
    "\n",
    "        x = self.relu(self.layer3(out))\n",
    "        x = x + out  # Use out-of-place operation\n",
    "\n",
    "        out = self.relu(self.layer4(x))\n",
    "        out = out + x  # Use out-of-place operation\n",
    "\n",
    "        x = self.relu(self.layer5(out))\n",
    "        x = x + out  # Use out-of-place operation\n",
    "\n",
    "        # Apply attention\n",
    "        x = self.attention(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepResidualMLPWithAttention(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid)\n",
    "model = DeepResidualMLPWithAttention(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001)\n",
    "model = DeepResidualMLPWithAttention(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.001, 0.01)\n",
    "model = DeepResidualMLPWithAttention(input_dim)\n",
    "train_network(model, cat_nu_cols, X_train, y_train, X_valid, y_valid, 0.0001, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择表现最好的模型进行最终训练和预测\n",
    "best_model = DeeperMLPModel(input_dim)\n",
    "\n",
    "print(\"使用全量数据训练最终模型...\")\n",
    "# 转换数据为tensor\n",
    "X_train_full_tensor = torch.tensor(X_train_full[cat_nu_cols].values, dtype=torch.float32)\n",
    "y_train_full_tensor = torch.tensor(y_train_full.values, dtype=torch.float32).view(-1, 1)\n",
    "train_dataset = TensorDataset(X_train_full_tensor, y_train_full_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 训练模型\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    best_model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "print(\"生成测试集预测结果...\")\n",
    "best_model.eval()\n",
    "X_test_tensor = torch.tensor(X_test[cat_nu_cols].values, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    test_predictions = best_model(X_test_tensor).numpy()\n",
    "\n",
    "# 创建预测结果DataFrame\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Id': range(len(test_predictions)),\n",
    "    'Predicted': test_predictions.flatten()\n",
    "})\n",
    "\n",
    "# 保存预测结果\n",
    "predictions_df.to_csv('data/predictions.csv', index=False)\n",
    "print(\"预测结果已保存到 data/predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
